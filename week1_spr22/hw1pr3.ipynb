{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### week1 ~ the-web-as-filesystem...   &nbsp;&nbsp; (hw1pr3.ipynb)\n",
    "\n",
    "[the google doc with hw1's details](https://docs.google.com/document/d/11ALzpsANe3ZDR5sk8-kgaElaX_fwlDINVlQ8WS8JfQE/edit)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Problem 3:  The \"Wisdom of the Web\":  Programming the web for inquiry!\n",
    "\n",
    "(hw1pr3.ipynb)\n",
    "\n",
    "+ Here, your task is to create a two-hop approach to \"inquire about\" a subject\n",
    "  + That is, a request, whose results are used to fashion a new request...\n",
    "  + whose results are interpreted/parsed into an answer to our original query.\n",
    "\n",
    "<br>\n",
    "\n",
    "Below is an example that uses Beautiful Soup -- that is not required.\n",
    "Feel free to use json-providing APIs (or other APIs providing structured data)\n",
    "\n",
    "<br>\n",
    "\n",
    "The assignment page has some example APIs, and there are many (many!) others!\n",
    "I encourage you to choose 1-2 of interest and explore...\n",
    "... and, from there, stitch together an \"inquiring system\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# see if you have the Beautiful Soup library...\n",
    "#\n",
    "\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# If you _don't_ have bs4, try installing it using pip:\n",
    "#    the pip you want is the one sharing the path of your python interpreter/kernel\n",
    "#    you can see this by clicking on the upper-left \"kernel\" pulldown\n",
    "#        for me, the kernel is /usr/local/bin/python3\n",
    "#                  so I'll use /usr/local/bin/pip3   (could be pip or pip3)\n",
    "\n",
    "#    !/usr/local/bin/pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hopefully, this now works! (if so, it will succeed silently)\n",
    "#            things to reset: (a) the python + jupyter extensions, (b) vscode itself\n",
    "\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this should work from pr1 and pr2...\n",
    "#\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this is the most popular HTML parser, lxml -- see if you have it (else install in the same way! :-)\n",
    "\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "cs35's example :-)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# get_school_colors_page(school)\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Here is our strategy:\n",
    "\n",
    "1. first we grab the pages that define each school's colors (from wikipedia)\n",
    "2. then, use bs4 to parse those pages and return a list of colors (we only handle pairs of colors)\n",
    "\n",
    "3. then, grab the page that defines the popularity of colors\n",
    "4. finally, use bs4 to compute a score for each school based on its colors  (our score == sum)\n",
    "\n",
    "That's it!  \n",
    "In each case, you get to decide if it's _higher_ scores or _lower_ scores that are better :-)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup     \n",
    "\n",
    "PARSER = \"html.parser\"     # not as flexible\n",
    "PARSER = \"lxml\"            # to use this, install with .../pip install lxml\n",
    "\n",
    "#\n",
    "# this gets a school's wikipedia page and then extracts its school colors\n",
    "#\n",
    "def get_school_colors_page(school):\n",
    "    \"\"\" get_school_colors_page takes in a string with a formatted  name\n",
    "        such as Harvey_Mudd_College or Stanford_University\n",
    "        \n",
    "        it tried to request the appropriate page from wikipedia and parse\n",
    "        it with Beautiful Soup - and it should return that soup object\n",
    "    \"\"\"\n",
    "    school_color_url = \"http://en.wikipedia.org/wiki/\" + school\n",
    "    response = requests.get(school_color_url)         # request the page\n",
    "    \n",
    "    if response.status_code == 404:                 # page not found\n",
    "        print(\"For the school\", school, \"There was a problem with getting the page\")\n",
    "        print(school_color_url)\n",
    "        \n",
    "    data_from_url = response.text                   # the HTML text from the page\n",
    "    soup = BeautifulSoup(data_from_url,PARSER)      # parsed with Beautiful Soup\n",
    "    return soup\n",
    "\n",
    "\n",
    "if False:\n",
    "    #school_2 = 'Harvey_Mudd_College'\n",
    "    school_2 = 'Pitzer_College'\n",
    "    # other options that have been tested and work...\n",
    "    #school_2 = 'Pomona_College'\n",
    "    #school_2 = 'Scripps_College'\n",
    "    #school_2 = 'Claremont_McKenna_College'\n",
    "    #school_2 = 'Stanford_University'\n",
    "    # not working:\n",
    "    #school_2 = 'Harvard_University'\n",
    "    result = get_school_colors_page(school_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_school_colors(soup):\n",
    "    \"\"\" extract_school_colors takes in a beautiful soup object, soup\n",
    "        and uses Beautiful Soup to extract a list of all of that school's colors\n",
    "        \n",
    "        it return that list of colors\n",
    "        \n",
    "        (Note that for different schools, you'll need to run the get_school_colors_page\n",
    "         to obtain soup objects for each page.)\n",
    "    \"\"\"\n",
    "\n",
    "    AllDivs = soup.findAll('a', attrs={\"title\":\"School colors\"})\n",
    "    # print(len(AllDivs))                 # debugging\n",
    "\n",
    "    for div in AllDivs:\n",
    "        ancestor = div.parent.parent      # this is the ancestor that holds the school colors (grandparent)\n",
    "        TDs = ancestor.findAll('td')      # it's in one of the <td> table cells </td>\n",
    "        Texts = [ x.text for x in TDs ]   # get the actual text\n",
    "\n",
    "        if len(Texts) < 1:                # didn't get any? a problem!\n",
    "            print(f\"No colors found!\")\n",
    "            return []\n",
    "\n",
    "        s = Texts[0]                      # take the first one...\n",
    "        colors = get_text_with_and(s)     # use our helper function\n",
    "        return colors                     # the colors... works many times, not every time.\n",
    "\n",
    "\n",
    "if False:\n",
    "    colors = extract_school_colors(result)\n",
    "    colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# a helper function to handle the messy alternatives, e.g., Black & gold, Blue and white\n",
    "# \n",
    "\n",
    "import string \n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "def get_text_with_and(s):\n",
    "    \"\"\" first, finds 'and' or '&' in s \n",
    "        then finds all of the alphabetic \n",
    "        characters in either direction from that conjunction!\n",
    "    \"\"\"\n",
    "    allowable = ' '+string.ascii_lowercase  # allowable letters\n",
    "    s = s.lower()                           # only consider lower case\n",
    "    if DEBUG: print(s)                                # for debugging\n",
    "    if ('and' in s or '&' in s) == False:   # is there an 'and' or '&'?  If not...\n",
    "        print(\"No 'and' found...\")          #    return the lowercased s\n",
    "        return s\n",
    "\n",
    "    and_index = s.find('and')               # Try 'and' first\n",
    "    if and_index == -1:                     # If it's not there,\n",
    "        and_index = s.find('&')             # Try '&'\n",
    "        if and_index == -1:                 # This should have been caught, but I'm superstitious\n",
    "            print(\"No 'and' found...\")\n",
    "            return s\n",
    "        left = s[:and_index]                # get the left and right pieces\n",
    "        right = s[and_index+1:]             # '&' is one char long, hence the 1 \n",
    "    else:\n",
    "        left = s[:and_index]                # get the left and right pieces\n",
    "        right = s[and_index+3:]             # 'and' is three chars long, hence the 3\n",
    "\n",
    "    # print(f\"left, right are {left,right}\") # for debugging -- now, get the color on the left and right:\n",
    "\n",
    "    new_left = ''\n",
    "    for c in left[::-1]:   # go backwards\n",
    "        if c not in allowable:\n",
    "            break\n",
    "        else:\n",
    "            new_left = c + new_left          # accumulate, as long as it's a character\n",
    "    left = new_left\n",
    "\n",
    "    new_right = ''\n",
    "    for c in right[:]:   # go forwards\n",
    "        if c not in allowable:\n",
    "            break\n",
    "        else:\n",
    "            new_right = new_right + c        # accumulate, as long as it's a character\n",
    "    right = new_right\n",
    "\n",
    "    left = left.strip()                      # remove whitespace on either side\n",
    "    right = right.strip()                    # remove whitespace on either side\n",
    "    #print(f\"left, right are now {left,right}\")  # more debugging!\n",
    "\n",
    "    return [left, right]                     # only two colors for this scraper!\n",
    "\n",
    "if False:\n",
    "    result = get_text_with_and(\"#$ this & that !?\")\n",
    "    print(f\"result is {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Color-page handling\n",
    "# \n",
    "\n",
    "# this line imports the HTML parser named BeautifulSoup:\n",
    "#      it's a class constructor; when called it will return an object of type BeautifulSoup\n",
    "from bs4 import BeautifulSoup     \n",
    "\n",
    "PARSER = \"html.parser\"     # not as flexible a parser\n",
    "PARSER = \"lxml\"            # to use lxml (the most common), you'll need to install with .../pip install lxml\n",
    "\n",
    "#\n",
    "# get_color_page()\n",
    "#\n",
    "def get_color_page():\n",
    "    \"\"\" This function requests the most-popular-colors page\n",
    "        and parses it with Beautiful Soup, returning the resulting\n",
    "        Beautiful Soup object, soup\n",
    "    \"\"\"\n",
    "    color_popularity_url = \"http://www.thetoptens.com/top-ten-favorite-colors/\"\n",
    "    response = requests.get(color_popularity_url)   # request the page\n",
    "\n",
    "    if response.status_code == 404:                 # page not found\n",
    "        print(\"There was a problem with getting the page:\")\n",
    "        print(color_popularity_url)\n",
    "\n",
    "    data_from_url = response.text                   # the HTML text from the page\n",
    "    #soup = BeautifulSoup(data_from_url,\"lxml\")      # parsed with Beautiful Soup\n",
    "    soup = BeautifulSoup(data_from_url,PARSER)      # parsed with Beautiful Soup\n",
    "    return soup\n",
    "\n",
    "\n",
    "#\n",
    "# find_color_score( color, soup )\n",
    "#\n",
    "def find_color_score( color_name, soup ):\n",
    "    \"\"\" find_color_score takes in color_name (a string represnting a color)\n",
    "        and soup, a Beautiful Soup object returned from a successful run of\n",
    "        get_color_page\n",
    "        \n",
    "        find_color_score returns our predictive model's number of points in\n",
    "        a potential match up involving a team with that color\n",
    "        \n",
    "        the number of points is 21 - ranking, where ranking is from 1 (most\n",
    "        popular color) to 20 (least popular color) or 21, representing all\n",
    "        of the others\n",
    "    \"\"\"\n",
    "    ListOfDivs = soup.findAll('div', {'class':\"i\"})   # the class name happens to be 'i' here...\n",
    "    ranking = 1\n",
    "    for div in ListOfDivs:\n",
    "        # print(div.em, div.b)                    # checking the subtags named em and b\n",
    "        this_divs_color = div.b.text.lower()      # getting the text from them (lowercase)\n",
    "        this_divs_ranking = 21                    # the deafult (integer) ranking: 21\n",
    "        try:\n",
    "            this_divs_ranking = int(ranking)      # try to convert it to an integer\n",
    "        except:                                   # if it fails\n",
    "            pass                                  # do nothing and leave it at 21\n",
    "        \n",
    "        if color_name == this_divs_color:         # check if we need to return this one\n",
    "            return this_divs_ranking\n",
    "        ranking +=1\n",
    "    # if we ran through the whole for loop without finding a match, the ranking is 21\n",
    "    return 21\n",
    "\n",
    "\n",
    "if True:\n",
    "    color_soup = get_color_page()      # should run this once!\n",
    "    color_to_test = \"blue\"\n",
    "    score = find_color_score(color_to_test,color_soup)\n",
    "    print(f\"score for {color_to_test} is {score}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# put it all together!\n",
    "#\n",
    "if True:\n",
    "    \"\"\"\n",
    "    # Here is an example of using the Web's Wisdom to answer the question\n",
    "    #\n",
    "    #     Which college is best.....?\n",
    "    #\n",
    "    #     Not limited to the 5Cs!\n",
    "    \"\"\"\n",
    "    #school_1 = 'Harvey_Mudd_College'\n",
    "    #school_1 = 'Pitzer_College'\n",
    "    school_1 = 'Scripps_College'\n",
    "    #other options (that have been tested and work)...\n",
    "    school_2 = 'Pomona_College'\n",
    "    #school_2 = 'Claremont_McKenna_College'\n",
    "    #school_2 = 'Stanford_University'\n",
    "    #school_2 = 'Harvard_University'\n",
    "    # \n",
    "    # As a reminder, here is our \"web scavenging\" approach:\n",
    "    #\n",
    "    # 1. first we grab the pages that define each school's colors (wikipedia)\n",
    "    # 2. then, use bs4 to parse those pages and return a list of colors\n",
    "    # 3. then, grab the page that defines the popularity of colors\n",
    "    # 4. finally, use bs4 to compute a score for each school based on its colors\n",
    "    #\n",
    "\n",
    "    # we get the school colors page for each school\n",
    "    # and we return a BeautifulSoup \"soup\" object for each!\n",
    "    school_soup_1 = get_school_colors_page(school_1)\n",
    "    school_soup_2 = get_school_colors_page(school_2)\n",
    "    print(\"Done scraping the school colors.\\n\")\n",
    "\n",
    "    # We have a function that actually grabs the colors from the page...\n",
    "    school_colors_1 = extract_school_colors( school_soup_1 )\n",
    "    school_colors_2 = extract_school_colors( school_soup_2 )\n",
    "    print()\n",
    "    print(\"School 1 (\" + school_1 + \") colors:\", school_colors_1)\n",
    "    print(\"School 2 (\" + school_2 + \") colors:\", school_colors_2)\n",
    "\n",
    "\n",
    "    # Next, we grab the color-popularity page (and parse it into\n",
    "    # a BeautifulSoup object...\n",
    "    # \n",
    "    color_popularity_soup = get_color_page()\n",
    "    print(\"\\nDone scraping the color-popularity page.\\n\")\n",
    "\n",
    "    # Finally, we convert the team colors into total scores\n",
    "    # which will reveal our predicted result\n",
    "    # Admittedly, our \"points\" are simply the ranking of how popular a color is.\n",
    "\n",
    "    # let's use a list comprehension as a reminder of how those work...\n",
    "    school_1_scores = [ find_color_score(clr, color_popularity_soup) for clr in school_colors_1 ]\n",
    "    school_2_scores = [ find_color_score(clr, color_popularity_soup) for clr in school_colors_2 ]\n",
    "    print(\"School 1 (\" + school_1 + \") scores:\", school_1_scores)\n",
    "    print(\"School 2 (\" + school_2 + \") scores:\", school_2_scores)\n",
    "    print()\n",
    "    print(\"School 1 (\" + school_1 + \") total score:\", sum(school_1_scores))\n",
    "    print(\"School 2 (\" + school_2 + \") total score:\", sum(school_2_scores))\n",
    "\n",
    "    #find the winner\n",
    "    if sum(school_1_scores) > sum(school_2_scores):\n",
    "        print(school_1, \" wins!\")\n",
    "    elif sum(school_1_scores) < sum(school_2_scores):\n",
    "        print(school_2, \" wins!\")\n",
    "    else:\n",
    "        print(\"It's a tie!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# your scavenger hunt - or question-answering - need not \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your task!     \n",
    "#\n",
    "\n",
    "#\n",
    "# Beautiful soup is not required!  Feel free to try it, if you'd like...\n",
    "# \n",
    "# Your task is to choose 1-2 new web APIs (or web pages) and then\n",
    "# create a two-step process that answers a question \n",
    "#      (as you see, both serious and not-so-serious questions are welcome!)\n",
    "# \n",
    "# That is, there should be a web-access (API or beautiful-soup-scraping) to obtain a first piece of information\n",
    "# then, use that to create a second web-access (same API, different API, or another raw page)\n",
    "# from which you construct your answer!\n",
    "#\n",
    "# As you see in this example, it does not _always_ need to work (Aargh! Harvard! ;-)\n",
    "#\n",
    "# But, it should work for several examples -- show off with a few of them :-)\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Your wisdom-of-the-web answerer ...\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ccb4bb6bd67730c9185e6c24c983362cd7b4575b595bfae100d8d91e48f4f1e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
